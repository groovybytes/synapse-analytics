{
	"name": "Expense_Category_Analysis",
	"properties": {
		"folder": {
			"name": "Analyses"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "453a2886-6c62-4c8f-be56-11e8cefe34b1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Notebook: Expense_Category_Analysis.ipynb\n",
					"\n",
					"# Import necessary libraries\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, expr, year, month, concat, lit, round, sum, avg, count\n",
					"import uuid\n",
					"from datetime import datetime"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Initialize Spark Session\n",
					"spark = SparkSession.builder.appName(\"Expense Category Analysis\").getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Read enriched data from Cosmos DB\n",
					"enriched_data = spark.read.format(\"cosmos.olap\") \\\n",
					"    .option(\"spark.synapse.linkedService\", \"CosmosDBLSproject-capstone\") \\\n",
					"    .option(\"spark.cosmos.container\", \"enriched\") \\\n",
					"    .load()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Filter for financial data with expenses\n",
					"financial_data = enriched_data.filter(\n",
					"    (col(\"entityType\") == \"financial_record\") & \n",
					"    (col(\"attributes.expenses.value\").isNotNull())\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# If we have real financial data, process it\n",
					"if financial_data.count() > 0:\n",
					"    # Process expense data by category (simulated for this demo)\n",
					"    from pyspark.sql.functions import from_json, schema_of_json\n",
					"    \n",
					"    # Assuming we have a JSON string of expense categories in the data\n",
					"    # Extract and parse it\n",
					"    expense_schema = \"map<string, double>\"\n",
					"    \n",
					"    expense_data = financial_data \\\n",
					"        .withColumn(\"timestamp\", col(\"attributes.date.value\").cast(\"timestamp\")) \\\n",
					"        .withColumn(\"year_month\", concat(year(col(\"timestamp\")), lit(\"-\"), month(col(\"timestamp\")))) \\\n",
					"        .withColumn(\"expense_categories\", expr(\"map('Utilities', attributes.expenses.value * 0.15, 'Rent', attributes.expenses.value * 0.35, 'Salaries', attributes.expenses.value * 0.4, 'Supplies', attributes.expenses.value * 0.1)\"))\n",
					"else:\n",
					"    # Create synthetic expense data for demo\n",
					"    expense_data = spark.createDataFrame([\n",
					"        (\"2025-01\", {\"Utilities\": 25000, \"Rent\": 35000, \"Salaries\": 80000, \"Supplies\": 15000}),\n",
					"        (\"2025-02\", {\"Utilities\": 24000, \"Rent\": 35000, \"Salaries\": 82000, \"Supplies\": 16000}),\n",
					"        (\"2025-03\", {\"Utilities\": 26000, \"Rent\": 35000, \"Salaries\": 81000, \"Supplies\": 14000})\n",
					"    ], [\"year_month\", \"expense_categories\"])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Explode the map to analyze each category\n",
					"from pyspark.sql.functions import explode"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"expense_categories = expense_data \\\n",
					"    .select(\"year_month\", explode(\"expense_categories\")) \\\n",
					"    .toDF(\"year_month\", \"category\", \"amount\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Aggregate by category\n",
					"category_totals = expense_categories \\\n",
					"    .groupBy(\"category\") \\\n",
					"    .agg(\n",
					"        sum(\"amount\").alias(\"total_amount\"),\n",
					"        avg(\"amount\").alias(\"average_monthly\"),\n",
					"        count(\"year_month\").alias(\"months_count\")\n",
					"    ) \\\n",
					"    .orderBy(col(\"total_amount\").desc())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Calculate category percentages of total\n",
					"total_expenses = category_totals.agg(sum(\"total_amount\")).collect()[0][0]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"category_analysis = category_totals \\\n",
					"    .withColumn(\"percentage\", round(col(\"total_amount\") / total_expenses * 100, 2)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Identify potential savings opportunities\n",
					"# For demo, we'll calculate a potential savings percentage for each category\n",
					"optimization_opportunities = category_analysis \\\n",
					"    .withColumn(\"potential_savings_percent\", \n",
					"                expr(\"case when category = 'Utilities' then 15 \" +\n",
					"                     \"when category = 'Supplies' then 12 \" +\n",
					"                     \"when category = 'Rent' then 5 \" +\n",
					"                     \"else 3 end\")) \\\n",
					"    .withColumn(\"potential_savings\", \n",
					"                round(col(\"total_amount\") * col(\"potential_savings_percent\") / 100, 2))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Prepare output in the required ProcessedData format\n",
					"now = datetime.now().isoformat()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Collect results\n",
					"category_results = category_analysis.collect()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"optimization_results = optimization_opportunities.collect()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Calculate total potential savings\n",
					"total_savings = optimization_opportunities.agg(sum(\"potential_savings\")).collect()[0][0]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Create processed data in the required format\n",
					"processed_data = {\n",
					"    \"id\": str(uuid.uuid4()),\n",
					"    \"projectId\": \"project-capstone\",\n",
					"    \"sourceIds\": financial_data.select(\"id\").distinct().limit(10).rdd.flatMap(lambda x: x).collect() or [\"sample-financial-data\"],\n",
					"    \"analysisType\": \"pattern\",\n",
					"    \"title\": \"Expense Category Analysis\",\n",
					"    \"description\": \"Analysis of expense categories and cost optimization opportunities\",\n",
					"    \"data\": {\n",
					"        \"type\": \"expense_analysis\",\n",
					"        \"value\": {\n",
					"            \"categories\": [{\n",
					"                \"name\": row.category,\n",
					"                \"total\": row.total_amount,\n",
					"                \"average_monthly\": row.average_monthly,\n",
					"                \"percentage\": row.percentage\n",
					"            } for row in category_results],\n",
					"            \"optimization\": [{\n",
					"                \"category\": row.category,\n",
					"                \"current_spend\": row.total_amount,\n",
					"                \"savings_percentage\": row.potential_savings_percent,\n",
					"                \"potential_savings\": row.potential_savings\n",
					"            } for row in optimization_results],\n",
					"            \"monthly_trends\": expense_categories.groupBy(\"year_month\").pivot(\"category\").sum(\"amount\").orderBy(\"year_month\").collect()\n",
					"        },\n",
					"        \"confidence\": 0.9\n",
					"    },\n",
					"    \"insights\": [\n",
					"        {\n",
					"            \"type\": \"cost_reduction\",\n",
					"            \"title\": \"Utilities Optimization\",\n",
					"            \"description\": \"Energy-efficient equipment and better monitoring could reduce utility costs by 15%\",\n",
					"            \"impact\": optimization_results[0].potential_savings if optimization_results[0].category == \"Utilities\" else total_savings * 0.4,\n",
					"            \"confidence\": 0.85\n",
					"        },\n",
					"        {\n",
					"            \"type\": \"cost_reduction\",\n",
					"            \"title\": \"Supplies Consolidation\",\n",
					"            \"description\": \"Bulk purchasing and vendor consolidation could reduce supply costs by 12%\",\n",
					"            \"impact\": optimization_results[1].potential_savings if len(optimization_results) > 1 and optimization_results[1].category == \"Supplies\" else total_savings * 0.25,\n",
					"            \"confidence\": 0.8\n",
					"        }\n",
					"    ],\n",
					"    \"visualizationType\": \"chart\",\n",
					"    \"visualizationConfig\": {\n",
					"        \"chartType\": [\"pie\", \"bar\"],\n",
					"        \"metrics\": [\"total\", \"percentage\", \"potential_savings\"]\n",
					"    },\n",
					"    \"processedAt\": now,\n",
					"    \"createdAt\": now\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Save to Data Lake\n",
					"processedDF = spark.createDataFrame([processed_data])\n",
					"processedDF.write.format(\"parquet\").mode(\"append\").save(\"abfss://processed@groovybytesdatalake.dfs.core.windows.net/expense_analysis/\")\n",
					"\n",
					"\n",
					"print(\"Expense category analysis completed and saved to Data Lake\")"
				],
				"execution_count": null
			}
		]
	}
}